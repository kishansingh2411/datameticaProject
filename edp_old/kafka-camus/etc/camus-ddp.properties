# Needed Camus properties, more cleanup to come
#
# Almost all properties have decent default properties. When in doubt, comment out the property.
#

# The Kafka brokers to connect to, format: kafka.brokers=host1:port,host2:port,host3:port
kafka.brokers=cvldhdpspk2.cscdev.com:6667,cvldhdpspk1.cscdev.com:6667

# The job name.
camus.job.name=Camus DDP Job

# final top-level data output directory, sub-directory will be dynamically created for each topic pulled
etl.destination.path=/edp/incoming/ddp/data

etl_destination_subdirs=incoming_ratecodes,incoming_ratepricearea,incoming_ratepricelevel,incoming_slsmn,incoming_rptctrs,incoming_code36,incoming_zipmaster,incoming_code999,incoming_code95,dwrvwr.controlparams,incoming_custrates,incoming_auxhouse,incoming_custbilladdr,incoming_wipoutlet,incoming_wipmaster,incoming_housemaster,incoming_auxcust,incoming_raterptctrs,incoming_custmaster,incoming_wipcustrate,incoming_boxinvtry,incoming_techs

# HDFS location where you want to keep execution files, i.e. offsets, error logs, and count files
etl.execution.base.path=/apps/camus/exec
# where completed Camus job output directories are kept, usually a sub-dir in the base.path
etl.execution.history.path=/apps/camus/exec/history

# Concrete implementation of the Encoder class to use (used by Kafka Audit, and thus optional for now)
#camus.message.encoder.class=com.linkedin.camus.etl.kafka.coders.DummyKafkaMessageEncoder

# Concrete implementation of the Decoder class to use.
# Out of the box options are:
#  com.linkedin.camus.etl.kafka.coders.JsonStringMessageDecoder - Reads JSON events, and tries to extract timestamp.
#  com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder - Reads Avro events using a schema from a configured schema repository.
#  com.linkedin.camus.etl.kafka.coders.LatestSchemaKafkaAvroMessageDecoder - Same, but converts event to latest schema for current topic.
#camus.message.decoder.class=io.confluent.camus.etl.kafka.coders.KafkaAvroMessageDecoder
#camus.message.decoder.class=io.confluent.camus.etl.kafka.coders.AvroMessageDecoder
camus.message.decoder.class=com.cablevision.edp.CustomAvroMessageDecoder
# Decoder class can also be set on a per topic basis.
#camus.message.decoder.class.<topic-name>=com.your.custom.MessageDecoder

# Used by avro-based Decoders (KafkaAvroMessageDecoder and LatestSchemaKafkaAvroMessageDecoder) to use as their schema registry.
# Out of the box options are:
# com.linkedin.camus.schemaregistry.FileSchemaRegistry
# com.linkedin.camus.schemaregistry.MemorySchemaRegistry
# com.linkedin.camus.schemaregistry.AvroRestSchemaRegistry
# com.linkedin.camus.example.schemaregistry.DummySchemaRegistry
kafka.message.coder.schema.registry.class=com.linkedin.camus.example.schemaregistry.DummySchemaRegistry

# Used by JsonStringMessageDecoder when extracting the timestamp
# Choose the field that holds the time stamp (default "timestamp")
#camus.message.timestamp.field=time
# What format is the timestamp in? Out of the box options are:
# "unix" or "unix_seconds": The value will be read as a long containing the seconds since epoc
# "unix_milliseconds": The value will be read as a long containing the milliseconds since epoc
# "ISO-8601": Timestamps will be fed directly into org.joda.time.DateTime constructor, which reads ISO-8601
# All other values will be fed into the java.text.SimpleDateFormat constructor, which will be used to parse the timestamps
# Default is "[dd/MMM/yyyy:HH:mm:ss Z]"
#camus.message.timestamp.format=yyyy-MM-dd_HH:mm:ss
#camus.message.timestamp.format=ISO-8601

# Used by the committer to arrange .avro files into a partitioned scheme. This will be the default partitioner for all
# topic that do not have a partitioner specified.
# Out of the box options are (for all options see the source for configuration options):
# com.linkedin.camus.etl.kafka.partitioner.HourlyPartitioner, groups files in hourly directories
# com.linkedin.camus.etl.kafka.partitioner.DailyPartitioner, groups files in daily directories
# com.linkedin.camus.etl.kafka.partitioner.TimeBasedPartitioner, groups files in very configurable directories
# com.linkedin.camus.etl.kafka.partitioner.DefaultPartitioner, like HourlyPartitioner but less configurable
# com.linkedin.camus.etl.kafka.partitioner.TopicGroupingPartitioner
#etl.partitioner.class=com.linkedin.camus.etl.kafka.partitioner.HourlyPartitioner
#etl.partitioner.class=com.cablevision.edp.FieldBasedPartitioner
etl.partitioner.fieldName=load_date

# Partitioners can also be set on a per-topic basis. (Note though that configuration is currently not per-topic.)
#etl.partitioner.class.<topic-name>=com.your.custom.CustomPartitioner

etl.partitioner.class.DDP_ORACLE_BOXINVTRY=com.cablevision.edp.FieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_WIPCUSTRATE=com.cablevision.edp.FieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_CUSTMASTER=com.cablevision.edp.FieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_HOUSEMASTER=com.cablevision.edp.FieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_CUSTRATES=com.cablevision.edp.FieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_WIPOUTLET=com.cablevision.edp.FieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_WIPMASTER=com.cablevision.edp.FieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_RATECODES=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_RATEPRICEAREA=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_RATEPRICELEVEL=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_SLSMN=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_RPTCTRS=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_CODE36=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_ZIPMASTER=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_CODE999=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_CODE95=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_AUXHOUSE=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_CUSTBILLADDR=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_AUXCUST=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_RATERPTCTRS=com.cablevision.edp.LimitedFieldBasedPartitioner
etl.partitioner.class.DDP_ORACLE_TECHS=com.cablevision.edp.LimitedFieldBasedPartitioner

# all files in this dir will be added to the distributed cache and placed on the classpath for hadoop tasks
# hdfs.default.classpath.dir=

# max hadoop tasks to use, each task can pull multiple topic partitions
mapred.map.tasks=21
# max historical time that will be pulled from each partition based on event timestamp
kafka.max.pull.hrs=-1
# events with a timestamp older than this will be discarded.
kafka.max.historical.days=-1
# Max minutes for each mapper to pull messages (-1 means no limit)
kafka.max.pull.minutes.per.task=-1

# if whitelist has values, only whitelisted topic are pulled. Nothing on the blacklist is pulled
kafka.blacklist.topics=
kafka.whitelist.topics=DDP_ORACLE_RATECODES,DDP_ORACLE_RATEPRICEAREA,DDP_ORACLE_RATEPRICELEVEL,DDP_ORACLE_SLSMN,DDP_ORACLE_RPTCTRS,DDP_ORACLE_CODE36,DDP_ORACLE_ZIPMASTER,DDP_ORACLE_CODE999,DDP_ORACLE_CODE95,DWRVWR.CONTROLPARAMS,DDP_ORACLE_CUSTRATES,DDP_ORACLE_AUXHOUSE,DDP_ORACLE_CUSTBILLADDR,DDP_ORACLE_WIPOUTLET,DDP_ORACLE_WIPMASTER,DDP_ORACLE_HOUSEMASTER,DDP_ORACLE_AUXCUST,DDP_ORACLE_RATERPTCTRS,DDP_ORACLE_CUSTMASTER,DDP_ORACLE_WIPCUSTRATE,DDP_ORACLE_BOXINVTRY,DDP_ORACLE_TECHS


log4j.configuration=true

# Name of the client as seen by kafka
kafka.client.name=camus
# Fetch request parameters:
#kafka.fetch.buffer.size=
#kafka.fetch.request.correlationid=
#kafka.fetch.request.max.wait=
#kafka.fetch.request.min.bytes=
#kafka.timeout.value=
kafka.move.to.earliest.offset=true
#Stops the mapper from getting inundated with Decoder exceptions for the same topic
#Default value is set to 10
max.decoder.exceptions.to.print=10

#Controls the submitting of counts to Kafka
#Default value set to true
post.tracking.counts.to.kafka=true
monitoring.event.class=class.that.generates.record.to.submit.counts.to.kafka

# everything below this point can be ignored for the time being, will provide more documentation down the road
##########################
etl.run.tracking.post=false
kafka.monitor.tier=
etl.counts.path=
kafka.monitor.time.granularity=10

etl.hourly=hourly
etl.daily=daily

# Should we ignore events that cannot be decoded (exception thrown by MessageDecoder)?
# `false` will fail the job, `true` will silently drop the event.
etl.ignore.schema.errors=false

# configure output compression for deflate or snappy. Defaults to deflate
mapred.output.compress=true
etl.output.codec=snappy

etl.default.timezone=America/Los_Angeles
etl.output.file.time.partition.mins=60
etl.keep.count.files=false
etl.execution.history.max.of.quota=.8

# Configures a customer reporter which extends BaseReporter to send etl data
etl.reporter.class=com.linkedin.camus.etl.kafka.reporter.StatsdReporter

mapred.map.max.attempts=1

kafka.client.buffer.size=20971520
kafka.client.so.timeout=60000

#zookeeper.session.timeout=
#zookeeper.connection.timeout=

schema.registry.url=http://cvldhdpan1.cscdev.com:8081
